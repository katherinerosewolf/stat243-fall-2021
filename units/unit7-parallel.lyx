#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[unicode=true]{hyperref}
\usepackage{/accounts/gen/vis/paciorek/latex/paciorek-asa,times,graphics}
\input{/accounts/gen/vis/paciorek/latex/paciorekMacros}
%\renewcommand{\baselinestretch}{1.5}
\hypersetup{unicode=true, pdfusetitle,bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true,}
\end_preamble
\use_default_options false
\begin_modules
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing onehalf
\use_hyperref false
\papersize letterpaper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Unit 7: Parallel Processing 
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

chunk_setup, include=FALSE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

library(knitr)
\end_layout

\begin_layout Plain Layout

library(parallel)
\end_layout

\begin_layout Plain Layout

read_chunk("unit7-parallel.R")
\end_layout

\begin_layout Plain Layout

read_chunk("unit7-parallel.py")
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
frame goal of unit around Student question
\end_layout

\begin_layout Plain Layout
check if I cover lazy/eager for future correctly in dask-future tutorial
\end_layout

\begin_layout Plain Layout
====
\end_layout

\begin_layout Plain Layout
read parallelR by Dirk: https://arxiv.org/abs/1912.11144
\end_layout

\begin_layout Plain Layout
see ~/staff/consults/savio/hero_ashman.txt for a meaty parallel R with memory,
 forking, multi-node/single-node usage question.
\begin_inset Newline newline
\end_inset

 example parallelization question - see Carlos paramo email 2021-04-19 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
References: 
\end_layout

\begin_layout Itemize
Tutorial on parallel processing using Python's Dask and R's future: 
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/berkeley-scf/tutorial-dask-future"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
This unit will be fairly Linux-focused as most serious parallel computation
 is done on systems where some variant of Linux is running.
 The single-machine parallelization discussed here should work on Macs and
 Windows, but some of the details of what is happening under the hood are
 different for Windows.
\end_layout

\begin_layout Section
Some scenarios for parallelization
\end_layout

\begin_layout Itemize
You need to fit a single statistical/machine learning model, such as a random
 forest or regression model, to your data.
 
\end_layout

\begin_layout Itemize
You need to fit three different statistical/machine learning models to your
 data.
 
\end_layout

\begin_layout Itemize
You are running a prediction method on 10 cross-validation folds, possibly
 using multiple statistical/machine learning models to do prediction.
\end_layout

\begin_layout Itemize
You are running an ensemble prediction method such as 
\emph on
SuperLearner
\emph default
 or 
\emph on
Bayesian model averaging
\emph default
 over 10 cross-validation folds, with 30 statistical/machine learning methods
 used for each fold.
 
\end_layout

\begin_layout Itemize
You are running stratified analyses on a very large dataset (e.g., running
 regression models once for each subgroup within a dataset).
\end_layout

\begin_layout Itemize
You are running a simulation study with n=1000 replicates.
 Each replicate involves fitting 10 statistical/machine learning methods.
 
\end_layout

\begin_layout Standard
Given you are in such a situation, can you do things in parallel? Can you
 do it on your laptop or a single computer? Will it be useful (i.e., faster
 or provide access to sufficient memory) to use multiple computers, such
 as multiple nodes in a Linux cluster? 
\end_layout

\begin_layout Standard
All of the functionality discussed in this Unit applies ONLY if the iterations/l
oops of your calculations can be done completely separately and do not depend
 on one another; i.e., you can do the computation as separate processes without
 communication between the processes.
 This scenario is called an 
\emph on
embarrassingly parallel
\emph default
 computation.
\end_layout

\begin_layout Subsection
Embarrassingly parallel (EP) problems
\end_layout

\begin_layout Standard
An EP problem is one that can be solved by doing independent computations
 in separate processes without communication between the processes.
 You can get the answer by doing separate tasks and then collecting the
 results.
 Examples in statistics include
\end_layout

\begin_layout Enumerate
simulations with many independent replicates
\end_layout

\begin_layout Enumerate
bootstrapping
\end_layout

\begin_layout Enumerate
stratified analyses
\end_layout

\begin_layout Enumerate
random forests
\end_layout

\begin_layout Enumerate
cross-validation.
\end_layout

\begin_layout Standard
The standard setup is that we have the same code running on different datasets.
 (Note that different processes may need different random number streams,
 as we will discuss in the Simulation Unit.)
\end_layout

\begin_layout Standard
To do parallel processing in this context, you need to have control of multiple
 processes.
 Note that on a shared system with queueing/scheduling software set up,
 this will generally mean requesting access to a certain number of processors
 and then running your job in such a way that you use multiple processors.
 
\end_layout

\begin_layout Standard
In general, except for some modest overhead, an EP problem can ideally be
 solved with 
\begin_inset Formula $1/p$
\end_inset

 the amount of time for the non-parallel implementation, given 
\begin_inset Formula $p$
\end_inset

 CPUs.
 This gives us a speedup of 
\begin_inset Formula $p$
\end_inset

, which is called linear speedup (basically anytime the speedup is of the
 form 
\begin_inset Formula $kp$
\end_inset

 for some constant 
\begin_inset Formula $k$
\end_inset

).
\end_layout

\begin_layout Section
Overview of parallel processing
\end_layout

\begin_layout Subsection
Computer architecture
\end_layout

\begin_layout Standard
Computers now come with multiple processors for doing computation.
 Basically, physical constraints have made it harder to keep increasing
 the speed of individual processors, so the chip industry is now putting
 multiple processing units in a given computer and trying/hoping to rely
 on implementing computations in a way that takes advantage of the multiple
 processors.
\end_layout

\begin_layout Standard
Everyday personal computers usually have more than one processor (more than
 one chip) and on a given processor, often have more than one core (multi-core).
 A multi-core processor has multiple processors on a single computer chip.
 On personal computers, all the processors and cores share the same memory.
\end_layout

\begin_layout Standard
Supercomputers and computer clusters generally have tens, hundreds, or thousands
 of 'nodes', linked by a fast local network.
 Each node is essentially a computer with its own processor(s) and memory.
 Memory is local to each node (distributed memory).
 One basic principle is that communication between a processor and its memory
 is much faster than communication between processors with different memory.
 An example of a modern supercomputer is the Cori supercomputer at Lawrence
 Berkeley National Lab, which has 12,076 nodes, and a total of 735,200 cores.
 Each node has either 96 or 128 GB of memory for a total of 1.3 PB of memory.
\end_layout

\begin_layout Standard
For our purposes, there is little practical distinction between multi-processor
 and multi-core situations.
 The main issue is whether processes share memory or not.
 In general, I won't distinguish between cores and processors.
 We'll just focus on the number of cores on given personal computer or a
 given node in a cluster.
\end_layout

\begin_layout Subsection
Some useful terminology:
\end_layout

\begin_layout Itemize

\emph on
cores
\emph default
: We'll use this term to mean the different processing units available on
 a single machine or node of a cluster.
\end_layout

\begin_layout Itemize

\emph on
nodes
\emph default
: We'll use this term to mean the different computers, each with their own
 distinct memory, that make up a cluster or supercomputer.
 
\end_layout

\begin_layout Itemize

\emph on
processes
\emph default
: instances of a program(s) executing on a machine; multiple processes may
 be executing at once.
 A given program may start up multiple processes at once.
 Ideally we have no more processes than cores on a node.
\end_layout

\begin_layout Itemize

\emph on
workers
\emph default
: the individual processes that are carrying out the (parallelized) computation.
 We'll use 
\emph on
worker
\emph default
 and 
\emph on
process
\emph default
 interchangeably.
 
\end_layout

\begin_layout Itemize

\emph on
tasks
\emph default
: individual units of computation; one or more tasks will be executed by
 a given process on a given core.
\end_layout

\begin_layout Itemize

\emph on
threads
\emph default
: multiple paths of execution within a single process; the OS sees the threads
 as a single process, but one can think of them as 'lightweight' processes.
 Ideally when considering the processes and their threads, we would the
 same number of cores as we have processes and threads combined.
 
\end_layout

\begin_layout Itemize

\emph on
forking
\emph default
: child processes are spawned that are identical to the parent, but with
 different process IDs and their own memory.
 In some cases if objects are not changed, the objects in the child process
 may refer back to the original objects in the original process, avoiding
 making copies.
\end_layout

\begin_layout Itemize

\emph on
sockets
\emph default
: some of R's parallel functionality involves creating new R processes (e.g.,
 starting processes via 
\emph on
Rscript
\emph default
) and communicating with them via a communication technology called sockets.
\end_layout

\begin_layout Itemize

\emph on
scheduler
\emph default
: a program that manages users' jobs on a cluster.
 
\end_layout

\begin_layout Itemize

\emph on
load-balanced
\emph default
: when all the cores that are part of a computation are busy for the entire
 period of time the computation is running.
\end_layout

\begin_layout Subsection
Distributed vs.
 shared memory
\end_layout

\begin_layout Standard
There are two basic flavors of parallel processing (leaving aside GPUs):
 distributed memory and shared memory.
 With shared memory, multiple processors (which I'll call cores for the
 rest of this document) share the same memory.
 With distributed memory, you have multiple nodes, each with their own memory.
 You can think of each node as a separate computer connected by a fast network.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
[see George's pdf for graphical representation, p.
 23]
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Shared memory
\end_layout

\begin_layout Standard
For shared memory parallelism, each core is accessing the same memory so
 there is no need to pass information (in the form of messages) between
 different machines.
 However, unless one is using threading (or in some cases when one has processes
 created by forking), objects will still be copied when creating new processes
 to do the work in parallel.
 With threaded computations, multiple threads can access object(s) without
 making explicit copies.
 But in some programming contexts one needs to be careful that the threads
 on different cores doesn't mistakenly overwrite places in memory that are
 used by other cores (this is not an issue in R).
\end_layout

\begin_layout Standard
We'll cover two types of shared memory parallelism approaches in this unit:
 
\end_layout

\begin_layout Itemize
threaded linear algebra 
\end_layout

\begin_layout Itemize
multicore functionality 
\end_layout

\begin_layout Paragraph
Threading
\end_layout

\begin_layout Standard
Threads are multiple paths of execution within a single process.
 If you are monitoring CPU usage (such as with 
\emph on
top
\emph default
 in Linux or Mac) and watching a job that is executing threaded code, you'll
 see the process using more than 100% of CPU.
 When this occurs, the process is using multiple cores, although it appears
 as a single process rather than as multiple processes.
\end_layout

\begin_layout Standard
Note that this is a different notion than a processor that is hyperthreaded.
 With hyperthreading a single core appears as two cores to the operating
 system.
\end_layout

\begin_layout Subsubsection
Distributed memory
\end_layout

\begin_layout Standard
Parallel programming for distributed memory parallelism requires passing
 messages between the different nodes.
 The standard protocol for doing this is MPI, of which there are various
 versions, including 
\emph on
openMPI
\emph default
.
 
\end_layout

\begin_layout Standard
While there are various R packages (e.g., 
\emph on
Rmpi
\emph default
 and the 
\emph on
pbdR
\emph default
 packages) that use MPI behind the scenes, we'll only cover distributed
 memory parallelization via the future package and Dask, which don't use
 MPI.
 
\end_layout

\begin_layout Subsection
Some other approaches to parallel processing
\end_layout

\begin_layout Subsubsection
GPUs
\end_layout

\begin_layout Standard
GPUs (Graphics Processing Units) are processing units originally designed
 for rendering graphics on a computer quickly.
 This is done by having a large number of simple processing units for massively
 parallel calculation.
 The idea of general purpose GPU (GPGPU) computing is to exploit this capability
 for general computation.
 
\end_layout

\begin_layout Standard
Most researchers don't program for a GPU directly but rather use software
 (often machine learning software such as Tensorflow, PyTorch, or Caffe)
 that has been programmed to take advantage of a GPU if one is available.
\end_layout

\begin_layout Subsubsection
Spark and Hadoop
\end_layout

\begin_layout Standard
Spark and Hadoop are systems for implementing computations in a distributed
 memory environment, using the MapReduce approach, as discussed in Unit
 8.
\end_layout

\begin_layout Subsubsection
Cloud computing
\end_layout

\begin_layout Standard
Amazon (Amazon Web Services' EC2 service), Google (Google Cloud Platform's
 Compute Engine service) and Microsoft (Azure) offer computing through the
 cloud.
 The basic idea is that they rent out their servers on a pay-as-you-go basis.
 You get access to a virtual machine that can run various versions of Linux
 or Microsoft Windows server and where you choose the number of processing
 cores you want.
 You configure the virtual machine with the applications, libraries, and
 data you need and then treat the virtual machine as if it were a physical
 machine that you log into as usual.
 You can also assemble multiple virtual machines into your own virtual cluster
 and use platforms such as Spark on the cloud provider's virtual machines.
 
\end_layout

\begin_layout Section
Parallelization strategies
\end_layout

\begin_layout Standard
Some of the considerations that apply when thinking about how effective
 a given parallelization approach will be include:
\end_layout

\begin_layout Itemize
the amount of memory that will be used by the various processes,
\end_layout

\begin_layout Itemize
the amount of communication that needs to happen – how much data will need
 to be passed between processes,
\end_layout

\begin_layout Itemize
the latency of any communication - how much delay/lag is there in sending
 data between processes or starting up a worker process, and
\end_layout

\begin_layout Itemize
to what extent do processes have to wait for other processes to finish before
 they can do their next step.
\end_layout

\begin_layout Standard
The following are some basic principles/suggestions for how to parallelize
 your computation.
\end_layout

\begin_layout Itemize
Should I use one machine/node or many machines/nodes?
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If you can do your computation on the cores of a single node using shared
 memory, that will be faster than using the same number of cores (or even
 somewhat more cores) across multiple nodes.
 Similarly, jobs with a lot of data/high memory requirements that one might
 think of as requiring Spark or Hadoop may in some cases be much faster
 if you can find a single machine with a lot of memory.
 
\end_layout

\begin_layout Itemize
That said, if you would run out of memory on a single node, then you'll
 need to use distributed memory.
\end_layout

\end_deeper
\begin_layout Itemize
What level or dimension should I parallelize over?
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If you have nested loops, you generally only want to parallelize at one
 level of the code.
 That said, in this unit we'll see some tools for parallelizing at multiple
 levels.
 Keep in mind whether your linear algebra is being threaded.
 Often you will want to parallelize over a loop and not use threaded linear
 algebra within the iterations of the loop.
 
\end_layout

\begin_layout Itemize
Often it makes sense to parallelize the outer loop when you have nested
 loops.
 
\end_layout

\begin_layout Itemize
You generally want to parallelize in such a way that your code is load-balanced
 and does not involve too much communication.
 
\end_layout

\end_deeper
\begin_layout Itemize
How do I balance communication overhead with keeping my cores busy?
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If you have very few tasks, particularly if the tasks take different amounts
 of time, often some processors will be idle and your code poorly load-balanced.
 
\end_layout

\begin_layout Itemize
If you have very many tasks and each one takes little time, the overhead
 of starting and stopping the tasks will reduce efficiency.
\end_layout

\end_deeper
\begin_layout Itemize
Should multiple tasks be pre-assigned (statically assigned) to a process
 (i.e., a worker) (sometimes called 
\emph on
prescheduling
\emph default
) or should tasks be assigned dynamically as previous tasks finish? 
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
To illustrate the difference, suppose you have 6 tasks and 3 workers.
 If the tasks are pre-assigned, worker 1 might be assigned tasks 1 and 4
 at the start, worker 2 assigned tasks 2 and 5, and worker 3 assigned tasks
 3 and 6.
 If the tasks are dynamically assigned, worker 1 would be assigned task
 1, worker 2 task 2, and worker 3 task 3.
 Then whichever worker finishes their task first (it woudn't necessarily
 be worker 1) would be assigned task 4 and so on.
 
\end_layout

\begin_layout Itemize
Basically if you have many tasks that each take similar time, you want to
 preschedule the tasks to reduce communication.
 If you have few tasks or tasks with highly variable completion times, you
 don't want to preschedule, to improve load-balancing.
 
\end_layout

\begin_layout Itemize
For R in particular, some of R's parallel functions allow you to say whether
 the tasks should be prescheduled.
 In the future package, 
\emph on
future_lapply
\emph default
 has arguments 
\emph on
future.scheduling
\emph default
 and 
\emph on
future.chunk.size
\emph default
.
 Similarly, there is the 
\emph on
mc.preschedule
\emph default
 argument in 
\emph on
mclapply()
\emph default
.
 
\end_layout

\end_deeper
\begin_layout Section
Introduction to the future package
\end_layout

\begin_layout Standard
Before we illustrate implementation of various kinds of parallelization,
 I'll give an overview of the 
\emph on
future
\emph default
 package, which we'll use for many of the implementations.
 The future package has been developed over the last few years and provides
 some nice functionality that is easier to use and more cohesive than the
 various other approaches to parallelization in R.
 
\end_layout

\begin_layout Standard
Other approaches include 
\emph on
parallel::parLapply
\emph default
, 
\emph on
parallel::mclapply
\emph default
, the use of 
\emph on
foreach
\emph default
 separate from 
\emph on
future
\emph default
, and the 
\emph on
partools
\emph default
 package.
 The 
\emph on
partools
\emph default
 package is interesting.
 It tries to take the parts of Spark/Hadoop most relevant for statistics-related
 work – a distributed file system and distributed data objects – and discard
 the parts that are a pain/not useful – fault tolerance when using many,
 many nodes/machines.
\end_layout

\begin_layout Subsection
Overview: Futures and the R future package
\end_layout

\begin_layout Standard
What is a 
\emph on
future
\emph default
? It's basically a flag used to tag a given operation such that when and
 where that operation is carried out is controlled at a higher level.
 If there are multiple operations tagged then this allows for parallelization
 across those operations.
\end_layout

\begin_layout Standard
According to Henrik Bengtsson (the 
\emph on
future
\emph default
 package developer) and those who developed the concept:
\end_layout

\begin_layout Itemize
a future is an abstraction for a value that will be available later 
\end_layout

\begin_layout Itemize
the value is the result of an evaluated expression 
\end_layout

\begin_layout Itemize
the state of a future is either unresolved or resolved
\end_layout

\begin_layout Standard
Why use futures? The 
\emph on
future
\emph default
 package allows one to write one's computational code without hard-coding
 whether or how parallelization would be done.
 Instead one writes the code in a generic way and at the beginning of one's
 code sets the 'plan' for how the parallel computation should be done given
 the computational resources available.
 Simply changing the 'plan' changes how parallelization is done for any
 given run of the code.
 
\end_layout

\begin_layout Standard
More concisely, the key ideas are:
\end_layout

\begin_layout Itemize
Separate what to parallelize from how and where the parallelization is actually
 carried out.
 
\end_layout

\begin_layout Itemize
Different users can run the same code on different computational resources
 (without touching the actual code that does the computation).
 
\end_layout

\begin_layout Subsection
Overview of parallel backends
\end_layout

\begin_layout Standard
One uses 
\emph on
plan()
\emph default
 to control how parallelization is done, including what machine(s) to use
 and how many cores on each machine to use.
\end_layout

\begin_layout Standard
For example,
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<multiprocess, eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

plan(multiprocess)  ## spreads work across multiple cores 
\end_layout

\begin_layout Plain Layout

# alternatively, one can also control number of workers 
\end_layout

\begin_layout Plain Layout

plan(multiprocess, workers = 4)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This table gives an overview of the different plans.
 
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Type
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Description
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Multi-node
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Copies of objects made?
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
multisession
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
uses additional R sessions as the workers
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
no
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
yes
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
multicore
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
uses forked R processes as the workers
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
no
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
not if object not modified
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
remote
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
uses an R process on another machine
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
yes
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
cluster
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
uses R sessions on other machine(s)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
yes
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Section
Illustrating the principles in specific case studies
\end_layout

\begin_layout Subsection
Scenario 1: one model fit
\end_layout

\begin_layout Standard

\series bold
Scenario
\series default
: You need to fit a single statistical/machine learning model, such as a
 random forest or regression model, to your data.
\end_layout

\begin_layout Subsubsection
Scenario 1A:
\end_layout

\begin_layout Standard
A given method may have been written to use parallelization and you simply
 need to figure out how to invoke the method for it to use multiple cores.
\end_layout

\begin_layout Standard
For example the documentation for the 
\emph on
randomForest
\emph default
 package doesn't indicate it can use multiple cores, but the 
\emph on
ranger
\emph default
 package can – note the 
\emph on
num.threads
\emph default
 argument.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<ranger>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Scenario 1B:
\end_layout

\begin_layout Standard
If a method does linear algebra computations on large matrices/vectors,
 R can call out to parallelized linear algebra packages (the BLAS and LAPACK).
\end_layout

\begin_layout Standard
The BLAS is the library of basic linear algebra operations (written in Fortran
 or C).
 A fast BLAS can greatly speed up linear algebra in R relative to the default
 BLAS that comes with R.
 Some fast BLAS libraries are
\end_layout

\begin_layout Itemize
Intel's 
\emph on
MKL
\emph default
; available for educational use for free
\end_layout

\begin_layout Itemize

\emph on
OpenBLAS
\emph default
; open source and free 
\end_layout

\begin_layout Itemize

\emph on
vecLib
\emph default
 for Macs; provided with your Mac
\end_layout

\begin_layout Standard
In addition to being fast when used on a single core, all of these BLAS
 libraries are threaded - if your computer has multiple cores and there
 are free resources, your linear algebra will use multiple cores, provided
 your program is linked against the threaded BLAS installed on your machine
 and provided the environment variable OMP_NUM_THREADS is not set to one.
 (Macs make use of VECLIB_MAXIMUM_THREADS rather than OMP_NUM_THREADS.)
\end_layout

\begin_layout Standard
Threading in R is limited to linear algebra, provided R is linked against
 a threaded BLAS.
\end_layout

\begin_layout Standard
Here's some code that illustrates the speed of using a threaded BLAS:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<R-linalg, eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Here the elapsed time indicates that using four threads gave us a two-three
 times (2-3x) speedup in terms of real time, while the user time indicates
 that the threaded calculation took a bit more total processing time (combining
 time across all processors) because of the overhead of using multiple threads.
 
\end_layout

\begin_layout Standard
Note that the code also illustrates use of an R package that can control
 the number of threads from within R, but you could also have set OMP_NUM_THREAD
S before starting R.
\end_layout

\begin_layout Standard
To use an optimized BLAS with R, talk to your systems administrator, see
 Section A.3 of the R Installation and Administration Manual (
\begin_inset CommandInset href
LatexCommand href
target "https://cran.r-project.org/manuals.html"
literal "false"

\end_inset

), or see these instructions to use vecLib BLAS from Apple's Accelerate
 framework on your own Mac:
\begin_inset Newline newline
\end_inset

 
\begin_inset CommandInset href
LatexCommand href
target "http://statistics.berkeley.edu/computing/blas"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
It's also possible to use an optimized BLAS with Python's 
\emph on
numpy
\emph default
 and 
\emph on
scipy
\emph default
 packages, on either Linux or using the Mac's 
\emph on
vecLib
\emph default
 BLAS.
 Details will depend on how you install Python, numpy, and scipy.
 
\end_layout

\begin_layout Subsection
Scenario 2: three different prediction methods on your data
\end_layout

\begin_layout Standard

\series bold
Scenario
\series default
: You need to fit three different statistical/machine learning models to
 your data.
\end_layout

\begin_layout Standard
What are some options?
\end_layout

\begin_layout Itemize
use one core per model 
\end_layout

\begin_layout Itemize
if you have rather more than three cores, apply the ideas here combined
 with Scenario 1 above - with access to a cluster and parallelized implementatio
ns of each model, you might use one node per model
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<basic-future>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Question: Why might this not have shown a perfect three-fold speedup?
\end_layout

\begin_layout Standard
You could also have used tools like 
\emph on
foreach
\emph default
 and 
\emph on
future_lapply
\emph default
 here as well, as we'll discuss next.
\end_layout

\begin_layout Subsection
Scenario 3: 10-fold CV and 10 or fewer cores
\end_layout

\begin_layout Standard

\series bold
Scenario
\series default
: You are running a prediction method on 10 cross-validation folds.
\end_layout

\begin_layout Standard
Here I'll illustrate parallel looping, using this simulated dataset and
 basic use of 
\emph on
randomForest()
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<rf-example>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Using a parallelized for loop with 
\emph on
foreach
\end_layout

\begin_layout Standard
The foreach package provides a 
\emph on
foreach
\emph default
 command that allows you to do this easily.
 foreach can use a variety of parallel ``back-ends'', of which the future
 package is one back-end (via the 
\emph on
doFuture
\emph default
 package) that provides a lot of flexibility in what computational resources
 are used via 
\emph on
plan()
\emph default
.
 For our purposes here, we'll focus on using shared memory cores.
 
\end_layout

\begin_layout Standard
Note that 
\emph on
foreach
\emph default
 also provides functionality for collecting and managing the results to
 avoid some of the bookkeeping you would need to do if writing your own
 standard for loop.
 The result of 
\emph on
foreach
\emph default
 will generally be a list, unless we request the results be combined in
 different way, using the 
\emph on
.combine
\emph default
 argument.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<foreach>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
You can debug by running serially using 
\family typewriter
%do%
\family default
 rather than 
\family typewriter
%dopar%
\family default
 or 
\family typewriter
%dorng%
\family default
.
 Note that you may need to load packages within the 
\emph on
foreach
\emph default
 construct to ensure a package is available to all of the calculations.
\end_layout

\begin_layout Subsubsection
Alternatively using parallel apply statements
\end_layout

\begin_layout Standard
The 
\emph on
future.apply
\emph default
 package also has the ability to parallelize the various 
\emph on
apply
\emph default
 functions (
\emph on
apply
\emph default
, 
\emph on
lapply
\emph default
, 
\emph on
sapply
\emph default
, etc.).
 
\end_layout

\begin_layout Standard
We'll consider parallel 
\emph on
future_lapply
\emph default
 and 
\emph on
future_sapply
\emph default
.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<future_lapply>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Question: why are the user time (and system time) miniscule when using 
\emph on
future_sapply?
\emph default
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
probably because the time spent in the worker processes (which are separate
 R processes) is not counted at the level of the overall master process
 that dispatches the workers.
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
Now suppose you have 4 cores (and therefore won't have an equal number of
 tasks per core).
 The approach in the next scenario should work better.
 
\end_layout

\begin_layout Subsection
Scenario 4: parallelizing over prediction methods
\end_layout

\begin_layout Standard

\series bold
Scenario
\series default
: parallelizing over prediction methods or other cases where execution time
 varies
\end_layout

\begin_layout Standard
If you need to parallelize over prediction methods or in other contexts
 in which the computation time for the different tasks varies widely, you
 want to avoid having the parallelization tool group the tasks in advance,
 because some cores may finish a lot more quickly than others.
 However, in many cases, this sort of grouping in advance (called prescheduling
 or 'static' allocation of tasks to workers) is the default.
 This is also the case with the future package – the default is to group
 the tasks in advance into 
\begin_inset Quotes eld
\end_inset

chunks
\begin_inset Quotes erd
\end_inset

, so that each worker processes one future (one chunk), containing multiple
 tasks.
 
\end_layout

\begin_layout Standard
First we'll set up an artificial example with four slow tasks and 12 fast
 tasks and see the speed of running with the default of prescheduling.
 Whether to preschedule or not is controlled by either the 
\emph on
future.scheduling
\emph default
 or 
\emph on
future.chunk.size
\emph default
 arguments.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<parallel-lapply-preschedule>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
And here we prevent prescheduling.
 I find the 
\emph on
future.chunk.size
\emph default
 argument easier to understand than the 
\emph on
future.scheduling
\emph default
 argument.
 
\emph on
future.chunk.size
\emph default
 says how many tasks to group together.
 So setting equal to 1 means no grouping and therefore not using static
 allocation.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<parallel-lapply-no-preschedule>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Scenario 5: 10-fold CV across multiple methods with many more than 10 cores
\end_layout

\begin_layout Standard

\series bold
Scenario
\series default
: You are running an ensemble prediction method such as SuperLearner or
 Bayesian model averaging on 10 cross-validation folds, with many statistical/ma
chine learning methods.
\end_layout

\begin_layout Standard
Here you want to take advantage of all the cores you have available, so
 you can't just parallelize over folds.
 
\end_layout

\begin_layout Standard
First we'll discuss how to deal with the nestedness of the problem and then
 we'll talk about how to make use of many cores across multiple nodes to
 parallelize over a large number of tasks.
\end_layout

\begin_layout Subsubsection
Scenario 5A: nested parallelization
\end_layout

\begin_layout Standard
One can always flatten the looping, either in a for loop or in similar ways
 when using apply-style statements.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<unnested-code, eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

## original code: multiple loops 
\end_layout

\begin_layout Plain Layout

for(fold in 1:n) { 
\end_layout

\begin_layout Plain Layout

  for(method in 1:M) { 
\end_layout

\begin_layout Plain Layout

     ### code here 
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

## revised code: flatten the loops 
\end_layout

\begin_layout Plain Layout

output <- foreach(idx = 1:(n*M)) %dopar% { 
\end_layout

\begin_layout Plain Layout

   fold <- idx %/% M + 1 
\end_layout

\begin_layout Plain Layout

   method <- idx %% M + 1 
\end_layout

\begin_layout Plain Layout

   ### code here 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Alternatively, 
\emph on
foreach
\emph default
 supports nested parallelization as follows:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<nested-foreach-code, eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

output <- foreach(fold = 1:n) %:% 
\end_layout

\begin_layout Plain Layout

  foreach(method = 1:M) %dopar% { 
\end_layout

\begin_layout Plain Layout

     ## code here 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The `%:%` basically causes the nesting to be flattened, with n*M total tasks
 run in parallel.
\end_layout

\begin_layout Standard
One can also use nested futures and the future package will just take care
 of parallelizing across all the individual tasks.
 I won't go into that here, but there is information in the tutorial.
\end_layout

\begin_layout Subsubsection
Scenario 5B: Parallelizing across multiple nodes
\end_layout

\begin_layout Standard
If you have access to multiple machines networked together, including a
 Linux cluster, you can use the tools in the future package across multiple
 nodes (either in a nested parallelization situation with many total tasks
 or just when you have lots of unnested tasks to parallelize over).
 Here we'll just illustrate how to use multiple nodes, but if you had a
 nested parallelization case you can combine the ideas just above with the
 use of multiple nodes.
\end_layout

\begin_layout Standard
Simply start R as you usually would.
 
\end_layout

\begin_layout Standard
Here we'll use foreach with the future-based doFuture backend.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<doFuture, eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To use future_lapply, set up the plan in similar fashion to above.
 You can then do:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<lapply-multiple-nodes, eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Scenario 6: Stratified analysis on a very large dataset
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
unit7 R code from 2017 shows explicit look at addresses of objects in this
 context
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Scenario
\series default
: You are doing stratified analysis on a very large dataset and want to
 avoid unnecessary copies.
\end_layout

\begin_layout Standard
In many of R's parallelization tools, if you try to parallelize this case
 on a single node, you end up making copies of the original dataset, which
 both takes up time and eats up memory.
 
\end_layout

\begin_layout Standard
Here when we use the 
\emph on
multisession
\emph default
 plan, we make copies for each worker.
 And it's even worse if we force each task to be sent separately so that
 there is one copy per task.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<parallel-copy, eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
However, if you are working on a single machine (i.e., with shared memory)
 you can avoid this by using parallelization strategies that fork the original
 R process (i.e., make a copy of the process) and use the big data objects
 in the global environment (yes, this violates the usual programming best
 practices of not using global variables).
 The 
\emph on
multicore
\emph default
 plan (not available on Windows) allows you to do this.
\end_layout

\begin_layout Standard
This creates R worker processes with the same state as the original R process.
 Interestingly, this means that global variables in the forked worker processes
 are just references to the objects in memory in the original R process.
 So the additional processes do not use additional memory for those objects
 (despite what is shown in top) and there is no time involved in making
 copies.
 However, if you modify objects in the worker processes then copies are
 made.
 
\end_layout

\begin_layout Standard
So here we avoid copying the original dataset.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<parallel-nocopy, eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Parallelizing across nodes requires copying any big data across machines
 (one can't fork processes across nodes), which will be slow.
\end_layout

\begin_layout Subsection
Scenario 7: Simulation study with n=1000 replicates: parallel random number
 generation
\end_layout

\begin_layout Standard
We won't cover this in class and you don't need to worry about this at the
 moment.
 Instead, I will mention the issue in the simulation unit when we talk about
 random number generation.
\end_layout

\begin_layout Standard
In Section 5.4, we set the random number seed to different values for random
 sample.
 One danger in setting the seed like that is that the random numbers in
 the different samples could overlap somewhat.
 This is probably somewhat unlikely if you are not generating a huge number
 of random numbers, but it's unclear how safe it is.
\end_layout

\begin_layout Standard
The key thing when thinking about random numbers in a parallel context is
 that you want to avoid having the same 'random' numbers occur on multiple
 processes.
 On a computer, random numbers are not actually random but are generated
 as a sequence of pseudo-random numbers designed to mimic true random numbers.
 The sequence is finite (but very long) and eventually repeats itself.
 When one sets a seed, one is choosing a position in that sequence to start
 from.
 Subsequent random numbers are based on that subsequence.
 All random numbers can be generated from one or more random uniform numbers,
 so we can just think about a sequence of values between 0 and 1.
 
\end_layout

\begin_layout Standard

\series bold
Scenario
\series default
: You are running a simulation study with n=1000 replicates.
\end_layout

\begin_layout Standard
Each replicate involves fitting two statistical/machine learning methods.
\end_layout

\begin_layout Standard
Here, unless you really have access to multiple hundreds of cores, you might
 as well just parallelize across replicates.
\end_layout

\begin_layout Standard
However, you need to think about random number generation.
 If you have overlap in the random numbers the replications may not be fully
 independent.
\end_layout

\begin_layout Standard
In R, the 
\emph on
rlecuyer
\emph default
 package deals with this.
 The L'Ecuyer algorithm has a period of 
\begin_inset ERT
status open

\begin_layout Plain Layout

$2^{191}$
\end_layout

\end_inset

, which it divides into subsequences of length 
\begin_inset ERT
status open

\begin_layout Plain Layout

$2^{127}$
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
Here's how you initialize independent sequences on different processes when
 using the
\emph on
 future_lapply
\emph default
.
 All you need to do is set the argument 
\emph on
future.seed
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<RNG-apply, eval=TRUE>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Dealing with parallel random number generation when using 
\emph on
foreach
\emph default
 or 
\emph on
future()
\emph default
 is a bit more involved.
 See the tutorial.
\end_layout

\begin_layout Section
Additional details and topics (optional)
\end_layout

\begin_layout Subsection
Setting the number of threads (cores used) in threaded code (including parallel
 linear algebra in R)
\end_layout

\begin_layout Standard
In general, threaded code will detect the number of cores available on a
 machine and make use of them.
 However, you can also explicitly control the number of threads available
 to a process.
 
\end_layout

\begin_layout Standard
For most threaded code (that based on the openMP protocol), the number of
 threads can be set by setting the OMP_NUM_THREADS environment variable
 (VECLIB_MAXIMUM_THREADS on a Mac).
 E.g., to set it for four threads in the bash shell:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<set-OMP, engine='bash', eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

export OMP_NUM_THREADS=4
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Do this before starting your R or Python session or before running your
 compiled executable.
 
\end_layout

\begin_layout Standard
Alternatively, you can set OMP_NUM_THREADS as you invoke your job, e.g., here
 with R:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<set-OMP2, engine='bash', eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

OMP_NUM_THREADS=4 R CMD BATCH --no-save job.R job.out
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Important warnings about use of threaded BLAS
\end_layout

\begin_layout Subsubsection
Speed and threaded BLAS
\end_layout

\begin_layout Standard
In many cases, using multiple threads for linear algebra operations will
 outperform using a single thread, but there is no guarantee that this will
 be the case, in particular for operations with small matrices and vectors.
 You can compare speeds by setting OMP_NUM_THREADS to different values.
 In cases where threaded linear algebra is slower than unthreaded, you would
 want to set OMP_NUM_THREADS to 1.
 
\end_layout

\begin_layout Standard
More generally, if you are using the parallel tools in Section 4 to simultaneous
ly carry out many independent calculations (tasks), it is likely to be more
 effective to use the fixed number of cores available on your machine so
 as to split up the tasks, one per core, without taking advantage of the
 threaded BLAS (i.e., restricting each process to a single thread).
 
\end_layout

\begin_layout Subsubsection
Conflicts between openBLAS and various R functionality
\end_layout

\begin_layout Standard
In the past, I've seen various issues arising when using threaded linear
 algebra.
 In some cases when the parallelization uses forking, I have seen cases
 where R hangs and doesn't finish the linear algebra calculation.
\end_layout

\begin_layout Standard
I've also seen a conflict between threaded linear algebra and R profiling
 (recall the discussion of profiling in Unit 4).
\end_layout

\begin_layout Standard
Some solutions are to set OMP_NUM_THREADS to 1 to prevent the BLAS from
 doing threaded calculations or to use parallelization approaches that avoid
 forking.
\end_layout

\begin_layout Section
Using Dask in Python
\end_layout

\begin_layout Standard
Dask has similar functionality to R's future package for parallelizing across
 one or more machines/nodes.
 In addition, it has the important feature of handling distributed datasets
 - datasets that are split into chunks/shareds and operated on in parallel.
 We'll see more about distributed datasets in Unit 8 but here we'll introduce
 the basic functionality.
\end_layout

\begin_layout Subsection
Scheduler
\end_layout

\begin_layout Standard
The scheduler is the analogue of plan in the R future package.
 For example to parallelize across multiple cores via separate Python processes,
 we'd do this.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<scheduler, engine='python', eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This table shows the different types of schedulers.
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Type
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Description
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Multi-node
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Copies of objects made?
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
synchronous
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
not in parallel (serial)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
threaded
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
threads within current Python session
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
processes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
background Python sessions
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
yes
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
distributed
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Python sessions across multiple nodes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
yes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
yes
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Comments:
\end_layout

\begin_layout Enumerate
Note that because of Python's Global Interpreter Lock (GIL) (which prevents
 threading of Python code), many computations done in pure Python code won't
 be parallelized using the 'threaded' scheduler; however computations on
 numeric data in numpy arrays, Pandas dataframes and other C/C++/Cython-based
 code will parallelize.
 
\end_layout

\begin_layout Enumerate
It's fine to use the distributed scheduler on one machine, such as your
 laptop.
 According to the Dask documentation, it has advantages over multiprocessing,
 including the diagnostic dashboard (see the tutorial) and better handling
 of when copies need to be made.
 In addition, one needs to use it for parallel map operations (see next
 section).
\end_layout

\begin_layout Subsection
Parallel map
\end_layout

\begin_layout Standard
This is the analog of apply/lapply/sapply type functions in R.
 The term 'map' here is a functional programming term (as well as having
 the same meaning as 'map' in the context of MapReduce).
\end_layout

\begin_layout Standard
To do a parallel map, we need to use the distributed scheduler, but it's
 fine to do that with multiple cores on a single machine (such as a laptop).
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<map, engine='python', eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The map operation appears to cache results.
 If you rerun the above with the same inputs, you get the same result back
 essentially instantaneously.
 HOWEVER, that means that if there is randomness in the results of your
 function for a given input, Dask will just continue to return the original
 output.
 
\end_layout

\begin_layout Subsection
Futures / delayed evaluation
\end_layout

\begin_layout Standard
The analog of using 
\emph on
future()
\emph default
 in R to delay/parallelize tasks is shown here.
 We use 
\emph on
delayed
\emph default
 to indicate the tasks and then to actually evaluate the code we need to
 run 
\emph on
compute
\emph default
.
 This is another example of lazy evaluation.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<delayed, engine='python', eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Final notes
\end_layout

\begin_layout Standard
I've only hit a few highlights here, in particular analogous functionality
 to the future package, but there's lots more details in the tutorial.
\end_layout

\begin_layout Standard
Some additional comments regarding the principles of parallelization already
 discussed:
\end_layout

\begin_layout Enumerate
You can set up nested parallelizations easily using 
\emph on
delayed
\emph default
.
\end_layout

\begin_layout Enumerate
Dask generally uses dynamic allocation (no prescheduling), which can be
 a drawback on some cases.
 You may want to manually break up computations into chunks in some cases.
\end_layout

\begin_layout Enumerate
You generally don't want to call 
\emph on
compute
\emph default
 separately for multiple steps of a computation, as Dask will generally
 avoid keeping things in memory.
 Instead, write out the code for all the steps and then call 
\emph on
compute
\emph default
 once.
\end_layout

\begin_layout Enumerate
Except with the 
\emph on
threads
\emph default
 scheduler, copies are made of all objects passed to the workers.
 However if you use the 
\emph on
distributed
\emph default
 scheduler, you can arrange things so one copy is sent for each worker (rather
 than for each task).
\end_layout

\begin_layout Enumerate
With a bit more work than in the future package in R, you can set up safe
 parallel random number generation.
\end_layout

\end_body
\end_document
